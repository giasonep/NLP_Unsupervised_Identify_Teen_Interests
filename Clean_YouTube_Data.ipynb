{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7e873b-0923-4868-9ed2-d89719960b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import contractions\n",
    "import warnings\n",
    "\n",
    "from my_functions import *\n",
    "from datetime import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e335882-eeb4-4259-bf8a-1f204980aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove chained assignment error message\n",
    "pd.options.mode.chained_assignment = None\n",
    "# ignore warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b08a97f2-7eb6-4619-b698-08e124bd2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign text and meta data csv's to pandas dataframes\n",
    "text_df = pd.read_csv(r'youtube_text_data.csv')\n",
    "meta_df = pd.read_csv(r'youtube_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0b9ba3-8964-4122-92a1-f4783b625506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1238, 2) (1520, 14)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates in both dataframes\n",
    "text_df = text_df.drop_duplicates(subset=['vidId'])\n",
    "meta_df = meta_df.drop_duplicates(subset=['vidId'])\n",
    "print(text_df.shape, meta_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dda99e6-dcd5-40c7-8e03-2839de6eb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge text and meta df for processing\n",
    "df = pd.merge(text_df, meta_df, how='left', on=\"vidId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "979f59a3-a7c7-44c4-ac27-7a378c2c562b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1238, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vidId</th>\n",
       "      <th>videoText</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>commentCount</th>\n",
       "      <th>vidLength</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>country</th>\n",
       "      <th>totChanViews</th>\n",
       "      <th>totSubscribers</th>\n",
       "      <th>totChanVideos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7PIMiDcwNvc</td>\n",
       "      <td>[Music] I am most of all happy and grateful t...</td>\n",
       "      <td>2019-08-24T15:39:57Z</td>\n",
       "      <td>Marzia &amp; Felix - Wedding 19.08.2019</td>\n",
       "      <td>39403934.0</td>\n",
       "      <td>5509252.0</td>\n",
       "      <td>33359.0</td>\n",
       "      <td>526815.0</td>\n",
       "      <td>PT6M22S</td>\n",
       "      <td>Our footage from the wedding, best day of my l...</td>\n",
       "      <td>PewDiePie</td>\n",
       "      <td>US</td>\n",
       "      <td>2.798403e+10</td>\n",
       "      <td>110000000.0</td>\n",
       "      <td>4443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2ZTZrghxvg</td>\n",
       "      <td>(upbeat mellow music) - Lilly, you're almost ...</td>\n",
       "      <td>2021-11-04T16:00:12Z</td>\n",
       "      <td>Indian Parents Give Me “The Talk.” It’s Not Wh...</td>\n",
       "      <td>125462.0</td>\n",
       "      <td>14308.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>PT3M40S</td>\n",
       "      <td>You’ve heard of the Kama Sutra. But allow me t...</td>\n",
       "      <td>Lilly Singh</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.489530e+09</td>\n",
       "      <td>14700000.0</td>\n",
       "      <td>836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ce4V_PYLhEo</td>\n",
       "      <td>(hoof beats clip-clopping) (spooky mellow mus...</td>\n",
       "      <td>2018-10-22T21:57:57Z</td>\n",
       "      <td>Halloween As An Adult vs A Kid</td>\n",
       "      <td>8157273.0</td>\n",
       "      <td>163837.0</td>\n",
       "      <td>3156.0</td>\n",
       "      <td>5633.0</td>\n",
       "      <td>PT2M31S</td>\n",
       "      <td>Remember when Halloween was an excuse to go bu...</td>\n",
       "      <td>Lilly Singh</td>\n",
       "      <td>-1</td>\n",
       "      <td>3.489530e+09</td>\n",
       "      <td>14700000.0</td>\n",
       "      <td>836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f6LCVGhRqx4</td>\n",
       "      <td>what's up guys welcome back to my youtube cha...</td>\n",
       "      <td>2021-10-24T16:00:31Z</td>\n",
       "      <td>WE WENT HERE FOR OUR 1 YEAR ANNIVERSARY...</td>\n",
       "      <td>623236.0</td>\n",
       "      <td>37788.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>1139.0</td>\n",
       "      <td>PT14M31S</td>\n",
       "      <td>Dixie and I traveled to Paris for fashion week...</td>\n",
       "      <td>Noah Beck</td>\n",
       "      <td>US</td>\n",
       "      <td>7.468230e+07</td>\n",
       "      <td>1590000.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dE-SHprUhYw</td>\n",
       "      <td>[Music] bunny checks from parents I must have...</td>\n",
       "      <td>2018-02-03T22:38:54Z</td>\n",
       "      <td>FUNNIEST TEXTS FROM PARENTS</td>\n",
       "      <td>4045355.0</td>\n",
       "      <td>94099.0</td>\n",
       "      <td>1198.0</td>\n",
       "      <td>6457.0</td>\n",
       "      <td>PT10M9S</td>\n",
       "      <td>FUNNIEST TEXTS FROM PARENTS! Funny text messag...</td>\n",
       "      <td>SSSniperWolf</td>\n",
       "      <td>US</td>\n",
       "      <td>1.618086e+10</td>\n",
       "      <td>30000000.0</td>\n",
       "      <td>2828.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         vidId                                          videoText  \\\n",
       "0  7PIMiDcwNvc   [Music] I am most of all happy and grateful t...   \n",
       "1  s2ZTZrghxvg   (upbeat mellow music) - Lilly, you're almost ...   \n",
       "2  ce4V_PYLhEo   (hoof beats clip-clopping) (spooky mellow mus...   \n",
       "3  f6LCVGhRqx4   what's up guys welcome back to my youtube cha...   \n",
       "4  dE-SHprUhYw   [Music] bunny checks from parents I must have...   \n",
       "\n",
       "                   date                                              title  \\\n",
       "0  2019-08-24T15:39:57Z                Marzia & Felix - Wedding 19.08.2019   \n",
       "1  2021-11-04T16:00:12Z  Indian Parents Give Me “The Talk.” It’s Not Wh...   \n",
       "2  2018-10-22T21:57:57Z                     Halloween As An Adult vs A Kid   \n",
       "3  2021-10-24T16:00:31Z         WE WENT HERE FOR OUR 1 YEAR ANNIVERSARY...   \n",
       "4  2018-02-03T22:38:54Z                        FUNNIEST TEXTS FROM PARENTS   \n",
       "\n",
       "        views      likes  dislikes  commentCount vidLength  \\\n",
       "0  39403934.0  5509252.0   33359.0      526815.0   PT6M22S   \n",
       "1    125462.0    14308.0     109.0         639.0   PT3M40S   \n",
       "2   8157273.0   163837.0    3156.0        5633.0   PT2M31S   \n",
       "3    623236.0    37788.0     469.0        1139.0  PT14M31S   \n",
       "4   4045355.0    94099.0    1198.0        6457.0   PT10M9S   \n",
       "\n",
       "                                         description       channel country  \\\n",
       "0  Our footage from the wedding, best day of my l...     PewDiePie      US   \n",
       "1  You’ve heard of the Kama Sutra. But allow me t...   Lilly Singh      -1   \n",
       "2  Remember when Halloween was an excuse to go bu...   Lilly Singh      -1   \n",
       "3  Dixie and I traveled to Paris for fashion week...     Noah Beck      US   \n",
       "4  FUNNIEST TEXTS FROM PARENTS! Funny text messag...  SSSniperWolf      US   \n",
       "\n",
       "   totChanViews  totSubscribers  totChanVideos  \n",
       "0  2.798403e+10     110000000.0         4443.0  \n",
       "1  3.489530e+09      14700000.0          836.0  \n",
       "2  3.489530e+09      14700000.0          836.0  \n",
       "3  7.468230e+07       1590000.0           49.0  \n",
       "4  1.618086e+10      30000000.0         2828.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1fb9e1-3694-432d-8899-3ce8d29ab0c8",
   "metadata": {},
   "source": [
    "#### Check if any channel is represented more than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ddb315-d89f-4dd0-9d66-9ee8abef211b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SSSniperWolf</th>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MrBeast</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nickelodeon</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TED-Ed</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colleen Ballinger</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mark Rober</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MrBeast Gaming</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lilly Singh</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dhar Mann</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZHC Crafts</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title\n",
       "channel                 \n",
       "SSSniperWolf         151\n",
       "MrBeast               38\n",
       "Nickelodeon           18\n",
       "TED-Ed                16\n",
       "Colleen Ballinger     16\n",
       "Mark Rober            16\n",
       "MrBeast Gaming        15\n",
       "Lilly Singh           15\n",
       "Dhar Mann             14\n",
       "ZHC Crafts            14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(df, index=['channel'], values='title', aggfunc=np.size).sort_values(\"title\", ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abd414-dce9-4a48-8c28-b526e669bc57",
   "metadata": {},
   "source": [
    "#### Reduce the amount of rows from SSSniperWolf and MrBeast  \n",
    "\n",
    "    NOTE: --> I later decided to keep all videos for more data and did not use the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0513414c-c0b1-4137-b23b-99635e030b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create SSSniperWolf mask\n",
    "# sniper_df = df[df['channel'] == 'SSSniperWolf']\n",
    "# # get a random df of SSniper Wolf rows to remove, we will get rid of 85% of SSSniperWolf's videos\n",
    "# remove_sniper_df = sniper_df.sample(125)\n",
    "# # assign index values to a list for removal\n",
    "# sniper_index_list = remove_sniper_df.index.values.tolist()\n",
    "# df.drop(sniper_index_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c026466-e07d-4eaa-870c-b7358ad3be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create MrBeast mask\n",
    "# beast_df = df[df['channel'] == 'MrBeast']\n",
    "# # get a random df of MrBeast rows to remove, we will get rid of 50% of MrBeast's videos\n",
    "# remove_beast_df = beast_df.sample(18)\n",
    "# # assign index values to a list for removal\n",
    "# beast_index_list = remove_beast_df.index.values.tolist()\n",
    "# df.drop(beast_index_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "997a0c02-74a5-4e57-a0d7-4079c6d68468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset the df's index\n",
    "# df = df[df['title'].notna()]\n",
    "# df = df[df['channel'] != 'Fox News']\n",
    "# df = df.reset_index(drop=True)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc593c5-a31f-4c3d-bdcf-67228dfe0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the pivot table now\n",
    "# pd.pivot_table(df, index=['channel'], values='title', aggfunc=np.size).sort_values(\"title\", ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf03f0c-3844-4613-a954-534e2f8ab3b2",
   "metadata": {},
   "source": [
    "### Clean and Create additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a043d25e-6c43-4bc4-8443-69cd669ca806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column that identifies if a video has music in it or not\n",
    "df['music'] = df.videoText.map(lambda x: 1 if '[Music]' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1b104a-9e3a-4ee9-a587-8702abe18e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only date from date column\n",
    "df['date'] = df['date'].apply(lambda x: str(x).split('T')[0])\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95634eb3-df70-428f-8fe8-53a0f0ac9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove videos before 2015\n",
    "df = (df[df['date'] > '2015-01-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3bdc471-a612-4bca-bd66-eda926abd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the video length column to a readable format\n",
    "df['vidLength'] = vidLengthConverter(df['vidLength'])  # custom function imported\n",
    "# convert vidLength to dateTime from string\n",
    "df['vidLength'] = df['vidLength'].map(lambda x: datetime.strptime(x, '%H:%M:%S').time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a10461-2918-4cbd-9ac2-6c6ac8f53692",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1b1a3d-15eb-452e-8f54-cec1faaf5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    # remove all words in brackets and parenthesis \n",
    "    # --> these are fillers that Youtube uses to identify general sounds like Music, Applause, etc., which are not helpful to our overall goal\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    # fix all contractions\n",
    "    text = contractions.fix(text)\n",
    "    # remove all puncuation\n",
    "    text = re.sub('[%s]'% re.escape(string.punctuation), '', text)\n",
    "    # strip and lowercase all letters\n",
    "    text = (re.sub('[\\W]+', ' ', text.strip().lower()))\n",
    "    return text + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a24ce56-8657-4a81-a328-fbc73773b158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['textCleaned'] = df['videoText'].map(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f8d967d-90d8-408a-be3b-32755943ca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i am most of all happy and grateful to finally...\n",
       "1    lilly you are almost old enough we think it is...\n",
       "2    yum these are my favorite oh god that is so sw...\n",
       "3    what is up guys welcome back to my youtube cha...\n",
       "4    bunny checks from parents i must have gotten t...\n",
       "Name: textCleaned, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textCleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6799616-3862-40fe-980e-b01a2eaacc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the df's index\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953be4bf-2c70-46c9-94cb-8859aede8aa2",
   "metadata": {},
   "source": [
    "### Explore Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59df25a-e828-422b-802a-2965bd388369",
   "metadata": {},
   "source": [
    "Remove all stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97c8c6b7-4341-4265-9bd8-78c68cb6849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to add additional words, use .union(['word']), \n",
    "# where word = a word you want to add as a stop word\n",
    "add_words = ['hmm', 'um', 'uh', 'oh', 'okay', 'yeah', 'la', 'cause', 'wanna', 'right', 'baby', 'gosh', 'isnt', 'hey', 'wait', 'the', 'ah']\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS.union(add_words)\n",
    "df['textCleaned'] = df['textCleaned'].apply(lambda x: ' '.join([word for word in x.split(' ') if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9ca268d-2532-4d37-bd4f-099ccaab9df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    happy grateful finally mighty hospital met tim...\n",
       "1    lilly old think time talk mom 32 years old chi...\n",
       "2    yum favorite god sweet ew god teeth covered mo...\n",
       "3    guys welcome youtube channel guy know just lef...\n",
       "4    bunny checks parents gotten stupid virus phone...\n",
       "Name: textCleaned, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textCleaned'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26c53d-8cd1-4360-be3e-fa9eca0b323f",
   "metadata": {},
   "source": [
    "Lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "411f9509-1435-4d49-b05a-813c9a3166f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    happy grateful finally mighty hospital met tim...\n",
       "1    lilly old think time talk mom 32 years old chi...\n",
       "2    yum favorite god sweet ew god teeth covered mo...\n",
       "3    guys welcome youtube channel guy know just lef...\n",
       "4    bunny checks parents gotten stupid virus phone...\n",
       "Name: textCleaned, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textCleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d5d66c5-4333-4cf3-b9c8-27d035a642a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "df['textCleaned'] = df['textCleaned'].map(lambda x: ' '.join([lemmer.lemmatize(word) for word in x.split(' ') if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7ad848d-6992-4835-a96f-670f11d1d3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    happy grateful finally mighty hospital met tim...\n",
       "1    lilly old think time talk mom 32 year old chir...\n",
       "2    yum favorite god sweet ew god teeth covered mo...\n",
       "3    guy welcome youtube channel guy know just left...\n",
       "4    bunny check parent gotten stupid virus phone s...\n",
       "Name: textCleaned, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['textCleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4562dcc1-a8f1-43b2-8f77-f150f75ec573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export full cleaned dataframe to csv\n",
    "df.to_csv('cleanedData.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e799ab0-92a5-458d-a688-8a437d3953ba",
   "metadata": {},
   "source": [
    "Create a channel based dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53f8851f-da99-432d-a702-c6e9229b46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "userText = df.groupby('channel').textCleaned.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5fa7d5e-9f9a-4446-8fa6-af0ff28b4b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "userText_df = pd.DataFrame(data=userText.values, index=userText.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "237f6c30-790c-40bb-8f6f-f242fb3a58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "userText_df.columns = ['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f8d3ccb-0d40-43e4-b0f0-35642bc5b810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channel</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1theK (원더케이)</th>\n",
       "      <td>wrong good expressing feeling warmhearted woma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABC News</th>\n",
       "      <td>ultimate key unlocking disruption contending a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AWESOME WORLD</th>\n",
       "      <td>day army trade mres like cheesecloth tried kor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AaronsAnimals</th>\n",
       "      <td>faster gaining catch sothis thing faster wooho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adam Norris</th>\n",
       "      <td>going push beasley video number 10 today slave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adobe Asia Pacific</th>\n",
       "      <td>hi everybody jeremy lord freelance illustrator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adventure Athlete</th>\n",
       "      <td>guy clark hazlit better known adventure athlet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airrack</th>\n",
       "      <td>america largest maze let like maze best friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aissata Amadou</th>\n",
       "      <td>hi guy welcome welcome channel today video goi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alan Becker</th>\n",
       "      <td>welcome window 7 computer generation people us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            documents\n",
       "channel                                                              \n",
       "1theK (원더케이)        wrong good expressing feeling warmhearted woma...\n",
       "ABC News            ultimate key unlocking disruption contending a...\n",
       "AWESOME WORLD       day army trade mres like cheesecloth tried kor...\n",
       "AaronsAnimals       faster gaining catch sothis thing faster wooho...\n",
       "Adam Norris         going push beasley video number 10 today slave...\n",
       "Adobe Asia Pacific  hi everybody jeremy lord freelance illustrator...\n",
       "Adventure Athlete   guy clark hazlit better known adventure athlet...\n",
       "Airrack             america largest maze let like maze best friend...\n",
       "Aissata Amadou      hi guy welcome welcome channel today video goi...\n",
       "Alan Becker         welcome window 7 computer generation people us..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userText_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32b8e729-18d9-4dd7-ab82-302c144e0740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1354"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(userText_df['documents'][425])   # ---> inspect a random grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7606d54-bb62-43e4-8026-b124619b1f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "userText_df.to_csv('channelData.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e73903-1cc0-487b-bfb8-7453c96b6d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
